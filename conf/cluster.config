/*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    foi-bioinformatics/nanometanf HPC Cluster Configuration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    High-performance computing cluster optimized settings for SLURM/PBS environments
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
*/

params {
    config_profile_name        = 'HPC Cluster'
    config_profile_description = 'High-performance computing cluster configuration for nanometanf'
    config_profile_contact     = 'andreas.sjodin@foi.se'
    config_profile_url         = 'https://github.com/foi-bioinformatics/nanometanf'

    // HPC-optimized settings
    max_cpus                   = 128
    max_memory                 = '1.TB'
    max_time                   = '72.h'
    
    // Cluster-specific optimizations
    optimization_profile       = 'high_throughput'
    enable_dynamic_resources   = true
    resource_safety_factor     = 0.85
    max_parallel_jobs          = 50
    
    // Enhanced performance monitoring
    enable_performance_logging = true
    enable_resource_monitoring = true
    resource_monitoring_interval = 60
}

process {
    // Default cluster settings
    executor      = 'slurm'
    queue         = 'normal'
    errorStrategy = { task.exitStatus in [130,143,137,104,134,139] ? 'retry' : 'finish' }
    maxRetries    = 2
    
    // Base resource allocation
    cpus   = 1
    memory = '8.GB'
    time   = '4.h'
    
    // Process-specific cluster configurations
    withLabel:process_single {
        cpus   = 1
        memory = '8.GB'
        time   = '4.h'
        queue  = 'short'
    }
    
    withLabel:process_low {
        cpus   = 4
        memory = '32.GB'
        time   = '8.h'
        queue  = 'normal'
    }
    
    withLabel:process_medium {
        cpus   = 16
        memory = '128.GB'
        time   = '24.h'
        queue  = 'normal'
    }
    
    withLabel:process_high {
        cpus   = 32
        memory = '256.GB'
        time   = '48.h'
        queue  = 'long'
    }
    
    withLabel:process_long {
        time   = '72.h'
        queue  = 'long'
    }
    
    withLabel:process_high_memory {
        memory = '512.GB'
        queue  = 'highmem'
    }
    
    withLabel:process_gpu {
        queue       = 'gpu'
        accelerator = 1
        clusterOptions = '--gres=gpu:1'
    }
    
    // nanometanf-specific HPC configurations
    withName:'DORADO_BASECALLER' {
        cpus    = 16
        memory  = '128.GB'
        time    = '48.h'
        queue   = 'gpu'
        accelerator = 2
        clusterOptions = '--gres=gpu:2 --exclusive'
        
        ext.args = [
            '--device cuda:all',
            '--emit_fastq',
            '--emit_sam',
            '--chunk_size 4000',
            '--overlap 500'
        ].join(' ')
    }
    
    withName:'KRAKEN2_KRAKEN2' {
        cpus    = 24
        memory  = '256.GB'
        time    = '24.h'
        queue   = 'highmem'
        
        ext.args = [
            '--threads ${task.cpus}',
            '--memory-mapping',
            '--confidence 0.1',
            '--minimum-base-quality 20'
        ].join(' ')
    }
    
    withName:'FASTP' {
        cpus    = 12
        memory  = '48.GB'
        time    = '12.h'
        queue   = 'normal'
        
        ext.args = [
            '--thread ${task.cpus}',
            '--split_prefix_digits 4',
            '--verbose'
        ].join(' ')
    }
    
    withName:'BLAST_BLASTN' {
        cpus    = 16
        memory  = '64.GB'
        time    = '24.h'
        queue   = 'normal'
        
        ext.args = [
            '-num_threads ${task.cpus}',
            '-max_target_seqs 100',
            '-evalue 1e-5'
        ].join(' ')
    }
    
    withName:'NANOPLOT' {
        cpus    = 8
        memory  = '32.GB'
        time    = '8.h'
        queue   = 'normal'
        
        ext.args = [
            '--threads ${task.cpus}',
            '--verbose'
        ].join(' ')
    }
    
    withName:'MULTIQC' {
        cpus    = 4
        memory  = '16.GB'
        time    = '4.h'
        queue   = 'short'
    }
    
    // Assembly processes for cluster
    withName:'FLYE' {
        cpus    = 32
        memory  = '256.GB'
        time    = '72.h'
        queue   = 'long'
        
        ext.args = [
            '--threads ${task.cpus}',
            '--iterations 3',
            '--meta'
        ].join(' ')
    }
    
    withName:'MINIASM' {
        cpus    = 16
        memory  = '128.GB'
        time    = '24.h'
        queue   = 'normal'
    }
    
    // Real-time processing optimizations
    withName:'.*REALTIME.*' {
        cpus    = 8
        memory  = '32.GB'
        time    = '12.h'
        queue   = 'priority'
        clusterOptions = '--priority=high'
    }
}

// Cluster-specific executor settings
executor {
    $slurm {
        queueSize    = 100
        submitRateLimit = '20 sec'
        pollInterval = '10 sec'
        
        // SLURM-specific job options
        jobName = { "nanometanf-${task.process}-${task.index}" }
        clusterOptions = '--export=ALL --account=nanometanf'
    }
    
    $pbs {
        queueSize    = 50
        submitRateLimit = '10 sec'
        pollInterval = '15 sec'
        
        // PBS-specific job options
        jobName = { "nanometanf_${task.process}_${task.index}" }
    }
}

// Enhanced cluster monitoring
trace {
    enabled = true
    file    = "${params.outdir}/pipeline_info/cluster_trace_${params.trace_report_suffix}.txt"
    fields  = 'task_id,hash,native_id,name,status,exit,submit,start,complete,duration,realtime,%cpu,%mem,rss,vmem,peak_rss,peak_vmem,rchar,wchar,syscr,syscw,read_bytes,write_bytes,queue,cpus,memory,disk,time,env'
}

// Cluster file system optimizations
singularity {
    enabled    = true
    autoMounts = true
    cacheDir   = '/shared/singularity/cache'
    
    // Use local scratch for improved I/O
    runOptions = '--bind /tmp:/tmp --bind /scratch:/scratch'
}

// Optimized work directory for cluster
workDir = '/scratch/${USER}/nanometanf_work'

// Module system integration (if available)
env {
    MODULEPATH = '/shared/modules'
}

// Cluster cleanup settings
cleanup = false  // Keep work files for debugging on cluster